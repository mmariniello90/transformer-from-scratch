{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b66c1f9-8f5d-4e1d-95a2-cb64427915b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d916e8c-08c2-44b6-a090-45040e604313",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e108b6-bfc8-4055-96ef-7e61a30c6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"love apple phones\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f405e9d-7199-4384-89e8-7d990392fb73",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8915bc0-e27b-4b83-8e23-47e40c33434d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'love', 1: 'apple', 2: 'phones'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text: str):\n",
    "    tokens = text.split()\n",
    "    tokens_id = range(len(tokens))\n",
    "    return tokens, tokens_id\n",
    "    \n",
    "vocab, tokens_id = tokenizer(text=sentence)\n",
    "tokens_map = dict(zip(tokens_id, vocab))\n",
    "\n",
    "tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820a10a-05ce-492e-a08e-5d4500c6bac1",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564bf6b5-e3b5-4c71-a87d-69b66c802ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8 # length of the embedding\n",
    "d_k = 4  # keys and quaries dimension\n",
    "d_v = 4  # values dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867b82e-2ebc-47bd-a53e-b0d30e03da0e",
   "metadata": {},
   "source": [
    "### Embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe1d38c-90b8-4b5e-ae72-86765283e41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78225753, 0.9884757 , 0.41644721, 0.71749692, 0.50666314,\n",
       "        0.84125382, 0.88550623, 0.17975176],\n",
       "       [0.51745202, 0.38937903, 0.17271229, 0.23723197, 0.03387767,\n",
       "        0.18519248, 0.75803969, 0.26412614],\n",
       "       [0.32629386, 0.63747084, 0.31457801, 0.26486692, 0.92467969,\n",
       "        0.49783837, 0.42522985, 0.78500256]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "id_embedding_map = {token: np.random.rand(d_model) for token in tokens_id}\n",
    "\n",
    "embeddings_matrix = np.stack(list(id_embedding_map.values()))\n",
    "\n",
    "display(embeddings_matrix)\n",
    "\n",
    "print(embeddings_matrix.shape) # len vocal * d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25aa17-8311-45a9-8dce-49caa906ffff",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62823e6-ee58-4734-a701-ca8c8fe7d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of shape (seq_len, d_model)\n",
    "    containing sinusoidal positional encodings.\n",
    "    \"\"\"\n",
    "    # positions: [0, 1, 2, ..., seq_len-1]\n",
    "    positions = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "    \n",
    "    # dimensions: [0, 1, 2, ..., d_model-1]\n",
    "    dims = np.arange(d_model)[np.newaxis, :]       # (1, d_model)\n",
    "    \n",
    "    # Calculate the 'div_term' = 10000^(2i / d_model) for even and odd indices\n",
    "    # We only need to handle the factor for the even dimension indices\n",
    "    # but we can do so by dividing dims//2 when forming the exponent:\n",
    "    div_term = np.power(10000.0, (dims // 2) * 2.0 / d_model)\n",
    "    \n",
    "    # Create an empty matrix for storing the encoding\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    # Even dims: use sine\n",
    "    pe[:, 0::2] = np.sin(positions / div_term[:, 0::2])\n",
    "    \n",
    "    # Odd dims: use cosine\n",
    "    pe[:, 1::2] = np.cos(positions / div_term[:, 1::2])\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be97119a-ab01-4715-b21d-60423f560212",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_matrix = sinusoidal_positional_encoding(seq_len=len(vocab), d_model=d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d1906-24d8-4497-817e-9a8caced61e7",
   "metadata": {},
   "source": [
    "### Adding Embeddings to Positional enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45f28598-154e-40be-aac1-e5e51c8d7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78225753 1.9884757  0.41644721 1.71749692 0.50666314 1.84125382\n",
      "  0.88550623 1.17975176]\n",
      " [1.358923   0.92968134 0.2725457  1.23223613 0.04387751 1.18514248\n",
      "  0.75903969 1.26412564]\n",
      " [1.23559129 0.22132401 0.51324734 1.2449335  0.94467835 1.49763838\n",
      "  0.42722984 1.78500056]]\n",
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.add(embeddings_matrix, pos_matrix)\n",
    "\n",
    "print(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3412754-e49c-4bfe-a82a-fd2bcfc76540",
   "metadata": {},
   "source": [
    "### Initialize W_k, W_q and W_v matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5e8d79b-0f2e-4773-8856-b0110459971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00581358 0.74509774 0.22289167 0.67983892]\n",
      " [0.67510815 0.89754159 0.95443399 0.79108334]\n",
      " [0.91539601 0.52155654 0.66244052 0.23199175]\n",
      " [0.92179022 0.74348323 0.09589523 0.83251177]\n",
      " [0.38519726 0.56646805 0.44867178 0.13805532]\n",
      " [0.1812258  0.0049433  0.43057946 0.79737343]\n",
      " [0.57006183 0.74437079 0.2164188  0.63903859]\n",
      " [0.18315223 0.35270595 0.62486562 0.91580085]]\n",
      "(8, 4)\n"
     ]
    }
   ],
   "source": [
    "# d_model * d_k\n",
    "# key[query] = value\n",
    "\n",
    "W_k = np.random.rand(d_model, d_k)\n",
    "W_q = np.random.rand(d_model, d_k)\n",
    "W_v = np.random.rand(d_model, d_k)\n",
    "\n",
    "print(W_k)\n",
    "print(W_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85f371-268a-4ae4-9181-66181b424ca1",
   "metadata": {},
   "source": [
    "### Create K, Q and V linear transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50360fee-4a28-4602-9d32-a37b289177b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.5610853  5.23308967 4.46175677 6.81571258]\n",
      " [2.91679301 3.94683834 2.97308667 5.34218524]\n",
      " [2.97976545 3.80268957 3.22257006 5.40289603]]\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# embeddings -> (3, 8), W_k -> (8, 4) = K -> (3, 4)\n",
    "\n",
    "K = embeddings @ W_k\n",
    "Q = embeddings @ W_q\n",
    "V = embeddings @ W_v\n",
    "\n",
    "print(K)\n",
    "print(K.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947cad9-adc0-4a93-bce4-74826c969522",
   "metadata": {},
   "source": [
    "### Similarity score via dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5632ba0-fd08-4274-94a2-4743e05048b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51.19151604 37.18700331 37.36968   ]\n",
      " [35.92473711 26.066561   26.21786946]\n",
      " [39.08212982 28.19236516 28.46137931]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# similarty = Q * K.T\n",
    "Q.shape, K.shape, K.T.shape\n",
    "\n",
    "similarity_matrix = (Q @ K.T) / np.sqrt(d_k)\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5bd01cf-5b98-48a1-aec5-5456858b782c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_love</th>\n",
       "      <th>Q_apple</th>\n",
       "      <th>Q_phones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K_love</th>\n",
       "      <td>51.191516</td>\n",
       "      <td>37.187003</td>\n",
       "      <td>37.369680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K_apple</th>\n",
       "      <td>35.924737</td>\n",
       "      <td>26.066561</td>\n",
       "      <td>26.217869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K_phones</th>\n",
       "      <td>39.082130</td>\n",
       "      <td>28.192365</td>\n",
       "      <td>28.461379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Q_love    Q_apple   Q_phones\n",
       "K_love    51.191516  37.187003  37.369680\n",
       "K_apple   35.924737  26.066561  26.217869\n",
       "K_phones  39.082130  28.192365  28.461379"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity = pd.DataFrame(sim_matrix, index=[f\"K_{token}\" for token in vocab], columns=[f\"Q_{token}\" for token in vocab])\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6753d-4c3b-49d3-b7c8-f0cd513caa10",
   "metadata": {},
   "source": [
    "### Convert is probabilities via softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e0af1bd-b4dc-4ecf-a0a2-de33e1af827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_love</th>\n",
       "      <th>Q_apple</th>\n",
       "      <th>Q_phones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K_love</th>\n",
       "      <td>9.999943e-01</td>\n",
       "      <td>0.999861</td>\n",
       "      <td>0.999850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K_apple</th>\n",
       "      <td>2.342716e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K_phones</th>\n",
       "      <td>5.507543e-06</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Q_love   Q_apple  Q_phones\n",
       "K_love    9.999943e-01  0.999861  0.999850\n",
       "K_apple   2.342716e-07  0.000015  0.000014\n",
       "K_phones  5.507543e-06  0.000124  0.000135"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = lambda x: np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "similarity_prob_matrix = softmax(similarity_matrix)\n",
    "\n",
    "df_similarity_prob = pd.DataFrame(similarity_prob_matrix, index=[f\"K_{token}\" for token in vocab], columns=[f\"Q_{token}\" for token in vocab])\n",
    "df_similarity_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23aac5a-8196-436b-ac9f-ef84abf73f3f",
   "metadata": {},
   "source": [
    "### Attention formula\n",
    "The attention mechanism can be expressed as:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q \\cdot K^\\top}{\\sqrt{d_k}}\\right) \\cdot V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3904f2e5-98ef-4286-8958-b793359f57b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.48622846e+01 1.28706430e+01 1.24085241e+01 1.40977763e+01]\n",
      " [1.30750430e-04 1.12927411e-04 1.16775130e-04 1.31763963e-04]\n",
      " [1.18125578e-03 1.02119570e-03 1.05899477e-03 1.19402658e-03]]\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# Attention -> softmax( (Q * K.T) / sqrt(d_k) ) * V\n",
    "attention = similarity_prob_matrix @ V\n",
    "\n",
    "print(attention)\n",
    "print(attention.shape) # number of vords * d_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d32bf-5729-467f-8501-8e87bb22f1ec",
   "metadata": {},
   "source": [
    "Each row is a new vector rapresentation of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e16b159-74b5-433d-bcc2-1bd6ccd4ff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new love: [14.86228465 12.87064303 12.40852415 14.09777628]\n",
      "new apple: [0.00013075 0.00011293 0.00011678 0.00013176]\n",
      "new phones: [0.00118126 0.0010212  0.00105899 0.00119403]\n"
     ]
    }
   ],
   "source": [
    "for t, e in zip(vocab, attention):\n",
    "    print(f\"new {t}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a038bb7-747f-42c6-9e6d-c7d0b8d9a37e",
   "metadata": {},
   "source": [
    "### Create W_o marrix for multi head computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ee72842-23eb-4d04-9d0f-9993d8490f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.81876408e-02 1.47190614e-01 2.54777067e-01 8.31229393e-01\n",
      "  2.72155422e-01 7.73214221e-01 1.74398622e-01 4.00250267e-01]\n",
      " [1.04187043e-01 2.15469075e-01 6.02959357e-01 2.26095797e-01\n",
      "  8.23316733e-01 6.15700419e-01 2.69758278e-01 8.64536500e-01]\n",
      " [6.17497537e-01 4.04103518e-01 8.82961718e-01 3.03137457e-01\n",
      "  2.63691187e-01 8.80932353e-01 4.64046835e-01 6.28353976e-01]\n",
      " [6.16391016e-01 6.52444226e-01 4.23397142e-01 1.25330180e-01\n",
      "  3.09311960e-01 4.21122019e-01 7.86015737e-01 3.62926700e-04]\n",
      " [5.76615409e-01 8.44129911e-01 2.88604904e-01 8.80840913e-01\n",
      "  7.83247161e-01 5.97249404e-01 5.06151501e-01 3.09687300e-01]\n",
      " [3.26017136e-01 6.68449682e-01 8.38887965e-01 3.94920585e-01\n",
      "  3.90236305e-01 9.83827641e-01 8.28383320e-01 3.51111890e-01]\n",
      " [2.90245148e-01 5.15568155e-01 9.70275721e-01 8.29719220e-01\n",
      "  1.76189210e-01 6.31311639e-01 5.65782863e-01 4.60448641e-01]\n",
      " [9.85958149e-01 4.65051727e-01 5.78449038e-01 9.04220429e-01\n",
      "  6.86995671e-01 7.89685412e-01 4.61571607e-01 5.38128121e-01]]\n",
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "W_o = np.random.rand(d_model, d_model)\n",
    "\n",
    "print(W_o)\n",
    "print(W_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638a296-b8ac-4b18-9c52-4731334fb040",
   "metadata": {},
   "source": [
    "### Ensamble all togheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "889df295-7b02-4463-887a-847a878b2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(\n",
    "    embeddings: np.array,\n",
    "    d_k: int,\n",
    "    heads_number: int):\n",
    "\n",
    "    embedding_size = embeddings.shape[1]\n",
    "    weights_size = (embedding_size, d_k)\n",
    "    \n",
    "    attention_res = []\n",
    "    for i in range(heads_number):\n",
    "        #np.random.seed(np.random.randint(1,1e4)+i)\n",
    "    \n",
    "        W_k = np.random.rand(weights_size[0], weights_size[1])\n",
    "        W_q = np.random.rand(weights_size[0], weights_size[1])\n",
    "        W_v = np.random.rand(weights_size[0], weights_size[1])\n",
    "\n",
    "        K = embeddings @ W_k\n",
    "        Q = embeddings @ W_q\n",
    "        V = embeddings @ W_v\n",
    "\n",
    "        sim_matrix = (Q @ K.T) / np.sqrt(d_k)\n",
    "    \n",
    "        softmax = lambda x: np.exp(x) / sum(np.exp(x))\n",
    "    \n",
    "        attention = softmax(sim_matrix) @ V\n",
    "        attention_res.append(attention)\n",
    "        \n",
    "    Z = np.concatenate(attention_res, axis=1)\n",
    "\n",
    "    if heads_number == 1:\n",
    "        return Z\n",
    "    else:\n",
    "        W_o = np.random.rand(embeddings.shape[1], embeddings.shape[1])\n",
    "        return Z @ W_o\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7f45d75-0700-4e52-b1ec-3d0f5f247968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.32699339e+01, 6.88208273e+01, 4.55828563e+01, 7.12309575e+01,\n",
       "         5.96603174e+01, 6.65213586e+01, 6.83997707e+01, 6.16278543e+01],\n",
       "        [3.62286484e-02, 4.82190764e-02, 3.40402108e-02, 3.09759029e-02,\n",
       "         2.71392674e-02, 3.43392635e-02, 4.84358002e-02, 4.18827041e-02],\n",
       "        [7.81874780e-01, 1.03953504e+00, 7.31190183e-01, 6.75879329e-01,\n",
       "         5.90705018e-01, 7.45611965e-01, 1.04374052e+00, 9.04007321e-01]]),\n",
       " (3, 8))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = multi_head_attention(\n",
    "    embeddings=embeddings,\n",
    "    d_k=4, \n",
    "    heads_number=2\n",
    ")\n",
    "\n",
    "res, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfc39f-00ca-4020-bd31-6e1ed60de98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-from-scratch-py3.11",
   "language": "python",
   "name": "transformer-from-scratch-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
